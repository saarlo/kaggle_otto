{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XBOOST_LOCATION = '/Users/siim/Lib/xgboost/wrapper'\n",
    "import sys\n",
    "sys.path.append(XBOOST_LOCATION)\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import scale, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Neural Nets\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportional_train_test_split(df, label_col, test_size=0.25):\n",
    "    '''\n",
    "    split dataframe into training and test set so that label proportions stay the same \n",
    "    @param df DataFrame input data\n",
    "    @param label_col String name of column that holds class labels\n",
    "    @test_size float proportion of test set in range (0,1)\n",
    "    '''   \n",
    "    label_col = 'target'\n",
    "    labels = df[label_col].unique()   \n",
    "    \n",
    "    train_arrays, test_arrays = [], []    \n",
    "    for label in labels:\n",
    "        arr1, arr2 = train_test_split(df[df[label_col] == label], test_size=test_size)\n",
    "        train_arrays.append(arr1)\n",
    "        test_arrays.append(arr2)        \n",
    "        \n",
    "    return pd.DataFrame(np.concatenate(train_arrays), columns=df.columns), \\\n",
    "            pd.DataFrame(np.concatenate(test_arrays), columns=df.columns)\n",
    "    \n",
    "   \n",
    "def llfun(act, pred):\n",
    "    '''log loss function from https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "    Differently from sklearn.metrics.log_loss does not require all classes\n",
    "    to be present in pred'''\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "\n",
    "def get_mean_prob(arr):\n",
    "    return np.mean( np.dstack(arr), axis=2)\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data from csv to DataFrames\n",
    "df_all = pd.read_csv('./train.csv', index_col=0)\n",
    "df_train, df_control = proportional_train_test_split(df_all, 'target', test_size=0.2)\n",
    "df_test = pd.read_csv('./test.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "#shuffle test and train data (for Neural Nets)\n",
    "df_all = df_all.reindex(np.random.permutation(df_all.index))\n",
    "labels_all = df_all.target.map(lambda x: int(x.split('_')[1]) - 1)\n",
    "\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "labels_train = df_train.target.map(lambda x: int(x.split('_')[1]) - 1)\n",
    "\n",
    "\n",
    "df_control = df_control.reindex(np.random.permutation(df_control.index))\n",
    "labels_control = df_control.target.map(lambda x: int(x.split('_')[1]) - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create xgboost data structures\n",
    "dtrain = xgb.DMatrix(df_train.drop('target', axis=1).values, \n",
    "                     label=[int(x.split('_')[1]) - 1 for x in df_train.target.values])\n",
    "\n",
    "dcontrol = xgb.DMatrix(df_control.drop('target', axis=1).values,\n",
    "                       label=[int(x.split('_')[1]) - 1 for x in df_control.target.values])\n",
    "\n",
    "dall = xgb.DMatrix(df_all.drop('target', axis=1).values, \n",
    "                   label=[int(x.split('_')[1]) - 1 for x in df_all.target.values]) \n",
    "\n",
    "dtest = xgb.DMatrix(df_test.values)\n",
    "\n",
    "\n",
    "#\n",
    "num_classes = 9\n",
    "num_features = 93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "#predictors_xgb, predictions_xgb = [], []\n",
    "\n",
    "sys.stdout.flush()\n",
    "param = {'bst:max_depth': 8, 'subsample': 0.9, \n",
    "         'bst:eta': 0.075, 'num_class': 9, \n",
    "         'silent': 0, 'eval_metric': 'mlogloss', \n",
    "         'objective': 'multi:softprob', \n",
    "         'colsample_bytree': 0.6, 'min_child_weight': 4}\n",
    "\n",
    "num_round = 550\n",
    "evallist = [(dcontrol,'eval'), (dtrain,'train')]\n",
    "\n",
    "for i in range(30):  \n",
    "    \n",
    "    time_start = time.time()\n",
    "    param['seed'] = np.random.randint(10000)\n",
    "    plst = param.items()\n",
    "    tmp1 = xgb.train( plst, dtrain, num_round, evallist )\n",
    "    predictors_xgb.append( tmp1 )\n",
    "    time_train = time.time() - time_start\n",
    "    \n",
    "    tmp2 = tmp1.predict(dcontrol) \n",
    "    predictions_xgb.append( tmp2 )\n",
    "    time_predict = time.time() - time_start - time_train\n",
    "    print '{}) {}, (time: {}, {})'.format(i, \n",
    "                                          log_loss(labels_control, tmp2),\n",
    "                                          time_train, time_predict)    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters \n",
    "iteration_params = [\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 200, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 200, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 600, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 600, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 800, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 800, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 512, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 512, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 1024, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 1024, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dropout0', DropoutLayer),\n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout1', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dropout0': 0.5,\n",
    "         'dense0_num_units': 1024, \n",
    "         'dropout1': 0.5,\n",
    "         'dense1_num_units': 1024, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('dense2', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 900, \n",
    "         'dropout0_p': 0.75,\n",
    "         'dense1_num_units': 900, \n",
    "         'dense2_num_units': 200, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('dropout1', DropoutLayer),           \n",
    "           ('dense2', DenseLayer), \n",
    "           ('dropout2', DropoutLayer),\n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 900, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 900, \n",
    "         'dropout1_p': 0.5,\n",
    "         'dense2_num_units': 900, \n",
    "         'dropout2_p': 0.75,\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('dropout1', DropoutLayer),           \n",
    "           ('dense2', DenseLayer), \n",
    "           ('dropout2', DropoutLayer),\n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 1024, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 512, \n",
    "         'dropout1_p': 0.5,\n",
    "         'dense2_num_units': 256, \n",
    "         'dropout2_p': 0.5,\n",
    "        }\n",
    "    ),\n",
    "    #####\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 512, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 512, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 600, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 600, \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "          [('input', InputLayer),             \n",
    "           ('dense0', DenseLayer), \n",
    "           ('dropout0', DropoutLayer),           \n",
    "           ('dense1', DenseLayer), \n",
    "           ('output', DenseLayer)], \n",
    "        {'dense0_num_units': 700, \n",
    "         'dropout0_p': 0.5,\n",
    "         'dense1_num_units': 700, \n",
    "        }\n",
    "    )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, scaler = preprocess_data( df_train[df_train.columns[:-1]].values.astype('float') )\n",
    "y = labels_train.values.astype('int32')\n",
    "#nets = [] \n",
    "\n",
    "for layers, params in iteration_params[-3:] * 10:\n",
    "    tmpNet = NeuralNet(layers=layers,\n",
    "                 input_shape=(None, num_features),\n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 eval_size=0.2,\n",
    "                 verbose=0,\n",
    "                 max_epochs=35,\n",
    "                 \n",
    "                 **params\n",
    "                 )\n",
    "    tmpNet.fit(X, y)\n",
    "    aux = min(tmpNet.train_history_, key=lambda x: x['valid_loss'])\n",
    "    print '{}) best @ epoch {}: {}'.format(len(nets), aux['epoch'], aux['valid_loss'])\n",
    "    sys.stdout.flush()\n",
    "    nets.append(tmpNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validate results\n",
    "X, _ = preprocess_data(df_control[df_control.columns[:-1]].values.astype('float'), scaler)\n",
    "predictions_nn = []\n",
    "for i, net in zip(range(len(nets)), nets[:]):    \n",
    "    pred = net.predict_proba( X )\n",
    "    ll = skl.metrics.log_loss(labels_control, pred)\n",
    "    \n",
    "    if ll > 0.51:\n",
    "        continue\n",
    "    print '{}) {}'.format(i, ll )\n",
    "    predictions_nn.append(pred)\n",
    "    \n",
    "print 'NN ({}) ensemble mean {}'.format(np.shape(predictions),\n",
    "    skl.metrics.log_loss(labels_control, get_mean_prob(predictions_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT\n",
      "PREDICT\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "skl_restart = False\n",
    "\n",
    "\n",
    "clfs = [\n",
    "    RandomForestClassifier(**params_rf),\n",
    "#    ExtraTreesClassifier(n_estimators=100),\n",
    "#    GradientBoostingClassifier(n_estimators=1000, learning_rate=0.5),\n",
    "#    SVC(C=10000, probability=True)\n",
    "]\n",
    "\n",
    "params_rf = {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 1000, \n",
    "            'min_samples_split': 3, 'criterion': 'gini', 'max_features': 10}\n",
    "if skl_restart:\n",
    "    predictors_skl = []\n",
    "    predictions_skl = []\n",
    "    \n",
    "print 'FIT'\n",
    "sys.stdout.flush()\n",
    "for clf in clfs*2:\n",
    "    predictors_skl.append( \n",
    "            clf.fit(df_train[df_train.columns[:-1]], labels_train) # convert classes to int 0...8                \n",
    "            )    \n",
    "\n",
    "print 'PREDICT'\n",
    "sys.stdout.flush()\n",
    "for clf in predictors_skl:\n",
    "    predictions_skl.append(clf.predict_proba( df_control[df_control.columns[:-1]] ))\n",
    "    \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate & Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK ens: 0.537128993152\n",
      "NN ens: 0.49128332543\n",
      "GB ens: 0.445642994198\n",
      "-- : 0.450854697643\n",
      "-- : 0.448594597719\n",
      "-- : 0.44981782105\n",
      "-- : 0.452437168303\n",
      "-- : 0.451266495907\n",
      "ENS 0.438570688446\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "review predictors and find ensemble combo\n",
    "'''\n",
    "nn_ens = get_mean_prob(predictions_nn)\n",
    "skl_ens = get_mean_prob(predictions_skl)\n",
    "xgb_ens = get_mean_prob(predictions_xgb)\n",
    "\n",
    "print 'SK ens:', log_loss(labels_control, skl_ens)\n",
    "print 'NN ens:', log_loss(labels_control, nn_ens)\n",
    "print 'GB ens:', log_loss(labels_control, xgb_ens)\n",
    "\n",
    "for prediction in predictions_xgb:\n",
    "    print '-- :', log_loss(labels_control, prediction)\n",
    "    \n",
    "print 'ENS', log_loss(labels_control, \n",
    "                      get_mean_prob([nn_ens * 0.3] + \n",
    "                                    [skl_ens * 0.0] +\n",
    "                                    [xgb_ens * 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREDICTIONS\n",
    "\n",
    "# Neural nets\n",
    "X, _ = preprocess_data( df_test.values.astype('float'), scaler)\n",
    "tests_nn = []\n",
    "for net in nets:\n",
    "    tests_nn.append( net.predict_proba( X ) )\n",
    "                               \n",
    "# XGB\n",
    "tests_xgb = []\n",
    "for bst in predictors_xgb[5:]:\n",
    "    tests_xgb.append( bst.predict( dtest ) )\n",
    "#test_xgb = bst.predict(dtest)\n",
    "\n",
    "# SKL, RF\n",
    "tests_rf = []\n",
    "for clf in predictors_skl:\n",
    "    tests_rf.append( clf.predict_proba( df_test.values) )\n",
    "    \n",
    "# Ensemble                           \n",
    "test_ens = get_mean_prob([aux * 0.3 for aux in tests_nn] +\n",
    "                            [aux * 0.7 for aux in tests_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_columns = ['Class_' + str(i) for i in range(1,10)]\n",
    "with open('./output_ens.csv', 'w') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['id'] + label_columns)\n",
    "    for i, r in enumerate(test_ens):\n",
    "        row = [i+1] + map(lambda x: round(x, 3), list(r))\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top predictions from each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process:\n",
    "- on first layer calculate probabilities with all classifiers\n",
    "- From each predictor of the 1st layer pick top N classes (that are above threshold)\n",
    "- Use those classes as features (total N * # of predictors) for 2nd layer (predicts probability)\n",
    "\n",
    "Result - initial try gave 0.86@LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate parameters:\n",
      "(144368, 9)\n",
      "(144368, 9)\n",
      "(144368, 9)\n",
      "(144368, 9)\n"
     ]
    }
   ],
   "source": [
    "print 'Generate parameters:'\n",
    "X = []\n",
    "tmp_preds = [get_mean_prob(tests_nn)] + \\\n",
    "            [get_mean_prob(tests_rf)] + \\\n",
    "            [get_mean_prob(tests_xgb)]\n",
    "      \n",
    "print np.shape(get_mean_prob(tests_nn))\n",
    "print np.shape(get_mean_prob(tests_rf))\n",
    "print np.shape(get_mean_prob(tests_xgb))\n",
    "\n",
    "#tmp_preds = [nn_ens] + [skl_ens] + [xgb_preds]\n",
    "for rows in zip(*tmp_preds):\n",
    "    args = []\n",
    "    for row in rows:\n",
    "        top_idxs = np.argsort(row)[-3:]        \n",
    "        for idx in top_idxs:\n",
    "            if row[idx] < 0.1:\n",
    "                args.append(-1)\n",
    "            else:\n",
    "                args.append(idx)        \n",
    "    X.append( args )\n",
    "print np.shape( X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%capture` not found (But cell magic `%%capture` exists, did you mean that instead?).\n",
      "[0]\ttrain-mlogloss:2.029082\n"
     ]
    }
   ],
   "source": [
    "%capture --no-stdout\n",
    "dtrain2 = xgb.DMatrix(X, labels_control)\n",
    "\n",
    "sys.stdout.flush()\n",
    "param = {'bst:max_depth': 8, 'subsample': 0.9, \n",
    "         'bst:eta': 0.075, 'num_class': 9, \n",
    "         'silent': 0, 'eval_metric': 'mlogloss', \n",
    "         'objective': 'multi:softprob', \n",
    "         'colsample_bytree': 0.6, 'min_child_weight': 4}\n",
    "plst = param.items()\n",
    "num_round = 600\n",
    "evallist = [(dtrain2,'train')]\n",
    "bst = xgb.train( plst, dtrain2, num_round, evallist )\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain2 = xgb.DMatrix( X )\n",
    "test_ens = bst.predict(dtrain2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis over classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here compare results by class and predictors\n",
    "\n",
    "- Find out best predictors per class, and use them for final decision\n",
    "- Just run new classifier over results of initial predictions\n",
    "- try first layer both with class and probability predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compare loglosses by class for each predictor\n",
    "'''\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[x] for x in labels])\n",
    "\n",
    "for cls in range(9):\n",
    "    lls = []\n",
    "    for prediction in predictions:\n",
    "        act = enc.transform([[x] for x in aux[aux == cls]]).toarray()\n",
    "        lls.append( llfun(act, prediction[aux == cls])[cls] ) \n",
    "    print 'llfun({})'.format(cls), [round(x, 3) for x in lls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL:\n",
      "1 0.261 3225\n",
      "5 0.228 2827\n",
      "7 0.137 1693\n",
      "2 0.129 1601\n",
      "8 0.08 991\n",
      "6 0.046 568\n",
      "4 0.044 548\n",
      "3 0.044 539\n",
      "0 0.031 386\n",
      "\n",
      "class, % of class among wrong, % of mistake in class\n",
      "1, 0.48 (1069) 0.086\n",
      "2, 0.2 (445) 0.036\n",
      "7, 0.097 (216) 0.017\n",
      "5, 0.075 (168) 0.014\n",
      "8, 0.067 (150) 0.012\n",
      "6, 0.036 (80) 0.006\n",
      "3, 0.026 (58) 0.005\n",
      "0, 0.012 (26) 0.002\n",
      "4, 0.006 (14) 0.001\n",
      "total mistake 2226\n",
      "\n",
      "class, % of class among wrong, % of mistake in class\n",
      "1, 0.514 (1196) 0.097\n",
      "2, 0.177 (413) 0.033\n",
      "7, 0.099 (231) 0.019\n",
      "5, 0.089 (207) 0.017\n",
      "8, 0.061 (142) 0.011\n",
      "6, 0.031 (72) 0.006\n",
      "3, 0.014 (32) 0.003\n",
      "0, 0.01 (23) 0.002\n",
      "4, 0.006 (13) 0.001\n",
      "total mistake 2329\n",
      "\n",
      "class, % of class among wrong, % of mistake in class\n",
      "1, 0.47 (1228) 0.099\n",
      "7, 0.191 (498) 0.04\n",
      "2, 0.152 (397) 0.032\n",
      "5, 0.055 (143) 0.012\n",
      "8, 0.048 (125) 0.01\n",
      "6, 0.042 (110) 0.009\n",
      "0, 0.027 (70) 0.006\n",
      "3, 0.011 (28) 0.002\n",
      "4, 0.006 (15) 0.001\n",
      "total mistake 2614\n",
      "\n",
      "class, % of class among wrong, % of mistake in class\n",
      "1, 0.442 (949) 0.077\n",
      "2, 0.223 (479) 0.039\n",
      "7, 0.081 (174) 0.014\n",
      "5, 0.07 (151) 0.012\n",
      "8, 0.067 (144) 0.012\n",
      "6, 0.047 (101) 0.008\n",
      "3, 0.037 (79) 0.006\n",
      "0, 0.026 (56) 0.005\n",
      "4, 0.007 (14) 0.001\n",
      "total mistake 2147\n",
      "\n",
      "\n",
      "\n",
      "best predictors per class\n",
      "0 [0.002, 0.002, 0.006, 0.005]\n",
      "1 [0.086, 0.097, 0.099, 0.077]\n",
      "2 [0.036, 0.033, 0.032, 0.039]\n",
      "3 [0.005, 0.003, 0.002, 0.006]\n",
      "4 [0.001, 0.001, 0.001, 0.001]\n",
      "5 [0.014, 0.017, 0.012, 0.012]\n",
      "6 [0.006, 0.006, 0.009, 0.008]\n",
      "7 [0.017, 0.019, 0.04, 0.014]\n",
      "8 [0.012, 0.011, 0.01, 0.012]\n"
     ]
    }
   ],
   "source": [
    "print 'TOTAL:'\n",
    "for a, b in df['4'].value_counts().iteritems():\n",
    "    print a, round(b/float(len(df)), 3), b\n",
    "\n",
    "print \n",
    "\n",
    "\n",
    "mistakes = {}\n",
    "\n",
    "for i in '0123':\n",
    "    print 'class, % of class among wrong, % of mistake in class'\n",
    "    mistakes[i], s = {}, []\n",
    "    for a, b in df[df[i] != df['4']][i].value_counts().iteritems():\n",
    "        #a - class\n",
    "        #b - wrong of that class\n",
    "        s.append(b)\n",
    "        print '{}, {} ({})'.format(a, round(b/float(sum(df[i] != df['4'])), 3), b),        \n",
    "        print round(b/float(len(df[i])), 3) #% of mistake in class\\\n",
    "        mistakes[i][a] = round(b/float(len(df[i])), 3)\n",
    "    print 'total mistake', sum(s)\n",
    "    print \n",
    "    \n",
    "print \n",
    "print\n",
    "print \"best predictors per class\"\n",
    "for cls in range(9):\n",
    "    print cls, [mistakes[i][cls] for i in '0123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
